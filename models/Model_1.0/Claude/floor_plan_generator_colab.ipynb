{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_header"
   },
   "source": [
    "# Floor Plan Generator Model Implementation\n",
    "\n",
    "**Date:** 2025-03-24\n",
    "**User:** MazharRehan\n",
    "\n",
    "This notebook implements a Conditional GAN model to generate floor plans based on the dataset of 5, 10, and 20 Marla plot designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "# Install any additional packages if needed\n",
    "!pip install -q matplotlib==3.5.1 tensorflow==2.8.0 scikit-learn==1.0.2 seaborn==0.11.2 tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Mount Google Drive to access your data\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep_section"
   },
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Define Parameters and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_params"
   },
   "source": [
    "# Define parameters\n",
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 16\n",
    "IMG_HEIGHT = 256  # Resized height\n",
    "IMG_WIDTH = 256   # Resized width\n",
    "NUM_CHANNELS = 3  # RGB\n",
    "EPOCHS = 200\n",
    "PLOT_TYPES = ['5_marla', '10_marla', '20_marla']\n",
    "# Number of output channels = number of room types + walls\n",
    "NUM_OUTPUT_CHANNELS = 24  # Based on your color coding scheme\n",
    "\n",
    "# Define dataset path (on Google Drive)\n",
    "DATASET_PATH = '/content/drive/MyDrive/FYP-FP-Generator-Model/dataset'  # Adjust this path to match your Drive structure\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR = '/content/output'\n",
    "CHECKPOINT_DIR = '/content/checkpoints'\n",
    "LOG_DIR = '/content/logs'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Define the color mapping for visualization\n",
    "COLOR_MAP = {\n",
    "    'Bedroom': [255, 0, 0],          # Red\n",
    "    'Bathroom': [0, 0, 255],         # Blue\n",
    "    'Kitchen': [255, 165, 0],        # Orange\n",
    "    'Drawing Room': [0, 128, 0],     # Green\n",
    "    'Garage': [165, 42, 42],         # Brown\n",
    "    'Lounge': [255, 255, 0],         # Yellow\n",
    "    'Backyard': [50, 205, 50],       # Lime Green\n",
    "    'Stairs': [0, 128, 128],         # Teal\n",
    "    'Storage': [128, 0, 128],        # Purple\n",
    "    'Open Space': [0, 255, 255],     # Cyan\n",
    "    'Prayer Room': [127, 127, 127],  # Crimson\n",
    "    'Staircase': [153, 51, 255],     # Violet\n",
    "    'Lobby': [255, 0, 255],          # Magenta\n",
    "    'Lawn': [64, 224, 208],          # Turquoise\n",
    "    'Dining': [225, 192, 203],       # Pink\n",
    "    'Servant': [75, 0, 130],         # Indigo\n",
    "    'Passage': [128, 128, 0],        # Olive Green\n",
    "    'Laundry': [230, 230, 250],      # Lavender\n",
    "    'Dressing': [255, 127, 80],      # Coral\n",
    "    'Side Garden': [255, 215, 0],    # Gold\n",
    "    'Library': [255, 191, 0],        # Amber\n",
    "    'Walls': [0, 0, 0],              # Black\n",
    "    'Door': [128, 0, 0],             # Mahogany\n",
    "    'Background': [255, 255, 255]    # White\n",
    "}\n",
    "\n",
    "# Create reverse mapping from RGB to room type index\n",
    "RGB_TO_INDEX = {}\n",
    "INDEX_TO_COLOR = []\n",
    "for i, (room_type, color) in enumerate(COLOR_MAP.items()):\n",
    "    RGB_TO_INDEX[tuple(color)] = i\n",
    "    INDEX_TO_COLOR.append(color)\n",
    "\n",
    "INDEX_TO_COLOR = np.array(INDEX_TO_COLOR)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading_section"
   },
   "source": [
    "### 2.2 Data Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loading_funcs"
   },
   "source": [
    "def load_dataset(dataset_path, plot_types=None):\n",
    "    \"\"\"\n",
    "    Load dataset from the given path for specified plot types.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the dataset directory\n",
    "        plot_types: List of plot types to include (e.g., ['5_marla', '10_marla'])\n",
    "        \n",
    "    Returns:\n",
    "        images: List of floor plan images\n",
    "        conditions: List of condition vectors (plot type one-hot encoded)\n",
    "    \"\"\"\n",
    "    if plot_types is None:\n",
    "        plot_types = PLOT_TYPES\n",
    "        \n",
    "    images = []\n",
    "    conditions = []\n",
    "    file_paths = []  # Store file paths for debugging\n",
    "    \n",
    "    for i, plot_type in enumerate(plot_types):\n",
    "        plot_dir = os.path.join(dataset_path, plot_type)\n",
    "        \n",
    "        # Skip if directory doesn't exist\n",
    "        if not os.path.exists(plot_dir):\n",
    "            print(f\"Warning: Directory {plot_dir} not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        files = [f for f in os.listdir(plot_dir) if f.endswith('.png')]\n",
    "        \n",
    "        print(f\"Found {len(files)} images in {plot_type} directory\")\n",
    "        \n",
    "        for file in tqdm(files):\n",
    "            file_path = os.path.join(plot_dir, file)\n",
    "            file_paths.append(file_path)\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            img = tf.keras.preprocessing.image.load_img(file_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            \n",
    "            # Normalize to [-1, 1]\n",
    "            img_array = (img_array / 127.5) - 1\n",
    "            \n",
    "            # Create one-hot encoding for plot type condition\n",
    "            condition = np.zeros(len(PLOT_TYPES))\n",
    "            condition[i] = 1\n",
    "            \n",
    "            images.append(img_array)\n",
    "            conditions.append(condition)\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        raise ValueError(f\"No images found in the dataset at {dataset_path}\")\n",
    "        \n",
    "    # Display a sample image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow((images[0] + 1) / 2)  # Convert from [-1, 1] to [0, 1] for display\n",
    "    plt.title(f\"Sample Image: {file_paths[0]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return np.array(images), np.array(conditions)\n",
    "\n",
    "def preprocess_image_to_segmentation_map(image):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to a segmentation map based on color mapping.\n",
    "    Each pixel will be assigned a class index based on its RGB value.\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image array with values in range [-1, 1]\n",
    "        \n",
    "    Returns:\n",
    "        segmentation_map: Array of class indices\n",
    "    \"\"\"\n",
    "    # Convert from [-1, 1] to [0, 255]\n",
    "    img = ((image + 1) * 127.5).astype(np.uint8)\n",
    "    \n",
    "    # Initialize segmentation map with background class\n",
    "    segmentation_map = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "    # Assign class indices based on RGB values\n",
    "    for y in range(img.shape[0]):\n",
    "        for x in range(img.shape[1]):\n",
    "            rgb = tuple(img[y, x])\n",
    "            # Find closest color in the color map\n",
    "            closest_color = min(RGB_TO_INDEX.keys(), \n",
    "                              key=lambda c: sum((c[i] - rgb[i])**2 for i in range(3)))\n",
    "            segmentation_map[y, x] = RGB_TO_INDEX[closest_color]\n",
    "    \n",
    "    return segmentation_map\n",
    "\n",
    "def preprocess_dataset(images, conditions):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for training.\n",
    "    \n",
    "    Args:\n",
    "        images: Array of floor plan images\n",
    "        conditions: Array of condition vectors\n",
    "        \n",
    "    Returns:\n",
    "        segmentation_maps: Array of segmentation maps\n",
    "        conditions: Array of condition vectors\n",
    "    \"\"\"\n",
    "    segmentation_maps = []\n",
    "    \n",
    "    for img in tqdm(images, desc=\"Creating segmentation maps\"):\n",
    "        segmentation_map = preprocess_image_to_segmentation_map(img)\n",
    "        segmentation_maps.append(segmentation_map)\n",
    "    \n",
    "    # Display a sample segmentation map\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(segmentation_maps[0], cmap='tab20')\n",
    "    plt.title(\"Sample Segmentation Map\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return np.array(segmentation_maps), conditions"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_section"
   },
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### 3.1 Loss Functions and Network Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_funcs"
   },
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    \"\"\"\n",
    "    Loss function for the generator.\n",
    "    \n",
    "    Args:\n",
    "        disc_generated_output: Discriminator output on generated images\n",
    "        gen_output: Generator output\n",
    "        target: Ground truth segmentation maps\n",
    "        \n",
    "    Returns:\n",
    "        total_loss: Combined adversarial and pixel-wise loss\n",
    "    \"\"\"\n",
    "    # Binary cross entropy loss for the GAN\n",
    "    gan_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # Pixel-wise loss (sparse categorical crossentropy for segmentation)\n",
    "    pixel_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(\n",
    "        target, gen_output)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = gan_loss + 100 * pixel_loss  # Higher weight on pixel loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    \"\"\"\n",
    "    Loss function for the discriminator.\n",
    "    \n",
    "    Args:\n",
    "        disc_real_output: Discriminator output on real images\n",
    "        disc_generated_output: Discriminator output on generated images\n",
    "        \n",
    "    Returns:\n",
    "        total_loss: Discriminator loss\n",
    "    \"\"\"\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.ones_like(disc_real_output), disc_real_output)\n",
    "    \n",
    "    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    total_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def upsample_block(x, filters, kernel_size=4, strides=2, apply_dropout=False):\n",
    "    \"\"\"\n",
    "    Upsampling block for the generator.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        filters: Number of filters\n",
    "        kernel_size: Size of the kernel\n",
    "        strides: Stride length\n",
    "        apply_dropout: Whether to apply dropout\n",
    "        \n",
    "    Returns:\n",
    "        x: Output tensor\n",
    "    \"\"\"\n",
    "    x = layers.Conv2DTranspose(filters, kernel_size, strides=strides, \n",
    "                              padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if apply_dropout:\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def downsample_block(x, filters, kernel_size=4, strides=2, apply_batchnorm=True):\n",
    "    \"\"\"\n",
    "    Downsampling block for the generator and discriminator.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        filters: Number of filters\n",
    "        kernel_size: Size of the kernel\n",
    "        strides: Stride length\n",
    "        apply_batchnorm: Whether to apply batch normalization\n",
    "        \n",
    "    Returns:\n",
    "        x: Output tensor\n",
    "    \"\"\"\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, \n",
    "                     padding='same', use_bias=False)(x)\n",
    "    \n",
    "    if apply_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generator_discriminator_section"
   },
   "source": [
    "### 3.2 Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build_models"
   },
   "source": [
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Build the generator model (U-Net architecture).\n",
    "    \n",
    "    Returns:\n",
    "        model: Generator model\n",
    "    \"\"\"\n",
    "    # Input: Random noise + condition vector\n",
    "    noise_input = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS])\n",
    "    condition_input = layers.Input(shape=[len(PLOT_TYPES)])\n",
    "    \n",
    "    # Expand condition to match spatial dimensions\n",
    "    condition_expanded = layers.Dense(IMG_HEIGHT * IMG_WIDTH)(condition_input)\n",
    "    condition_expanded = layers.Reshape((IMG_HEIGHT, IMG_WIDTH, 1))(condition_expanded)\n",
    "    \n",
    "    # Concatenate noise and condition\n",
    "    x = layers.Concatenate()([noise_input, condition_expanded])\n",
    "    \n",
    "    # Encoder\n",
    "    # For simplicity, we apply an initial convolution to combine the inputs\n",
    "    x = layers.Conv2D(64, 4, strides=1, padding='same')(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # Downsampling blocks\n",
    "    encoder_outputs = []\n",
    "    down_filters = [64, 128, 256, 512, 512, 512]\n",
    "    \n",
    "    for filters in down_filters:\n",
    "        x = downsample_block(x, filters)\n",
    "        encoder_outputs.append(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = downsample_block(encoder_outputs[-1], 512)\n",
    "    \n",
    "    # Decoder with skip connections\n",
    "    up_filters = [512, 512, 512, 256, 128, 64]\n",
    "    apply_dropout = [True, True, True, False, False, False]\n",
    "    \n",
    "    x = bottleneck\n",
    "    \n",
    "    for i, (filters, dropout) in enumerate(zip(up_filters, apply_dropout)):\n",
    "        x = upsample_block(x, filters, apply_dropout=dropout)\n",
    "        # Skip connection (except for the last layer)\n",
    "        if i < len(encoder_outputs):\n",
    "            x = layers.Concatenate()([x, encoder_outputs[-(i+1)]])\n",
    "    \n",
    "    # Output layer (num channels = number of room types)\n",
    "    output = layers.Conv2DTranspose(NUM_OUTPUT_CHANNELS, 4, strides=1, \n",
    "                                   padding='same', activation=None)(x)\n",
    "    \n",
    "    return models.Model([noise_input, condition_input], output, name='generator')\n",
    "\n",
    "def build_discriminator():\n",
    "    \"\"\"\n",
    "    Build the discriminator model (PatchGAN).\n",
    "    \n",
    "    Returns:\n",
    "        model: Discriminator model\n",
    "    \"\"\"\n",
    "    # Input: Generated or real image + condition vector\n",
    "    image_input = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS])\n",
    "    condition_input = layers.Input(shape=[len(PLOT_TYPES)])\n",
    "    \n",
    "    # Expand condition to match spatial dimensions\n",
    "    condition_expanded = layers.Dense(IMG_HEIGHT * IMG_WIDTH)(condition_input)\n",
    "    condition_expanded = layers.Reshape((IMG_HEIGHT, IMG_WIDTH, 1))(condition_expanded)\n",
    "    \n",
    "    # Concatenate image and condition\n",
    "    x = layers.Concatenate()([image_input, condition_expanded])\n",
    "    \n",
    "    # Downsampling blocks\n",
    "    filters = [64, 128, 256, 512]\n",
    "    \n",
    "    for i, f in enumerate(filters):\n",
    "        # For the first layer, don't apply batch normalization\n",
    "        x = downsample_block(x, f, apply_batchnorm=(i != 0))\n",
    "    \n",
    "    # Output layer (PatchGAN)\n",
    "    output = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
    "    \n",
    "    return models.Model([image_input, condition_input], output, name='discriminator')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "floor_plan_generator_class_section"
   },
   "source": [
    "## 4. FloorPlanGenerator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "floor_plan_generator_class"
   },
   "source": [
    "class FloorPlanGenerator:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.generator = build_generator()\n",
    "        self.discriminator = build_discriminator()\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.discriminator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        \n",
    "        # Setup checkpoint\n",
    "        self.checkpoint_dir = CHECKPOINT_DIR\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            generator_optimizer=self.generator_optimizer,\n",
    "            discriminator_optimizer=self.discriminator_optimizer,\n",
    "            generator=self.generator,\n",
    "            discriminator=self.discriminator\n",
    "        )\n",
    "        \n",
    "        # Initialize logs\n",
    "        self.log_dir = LOG_DIR\n",
    "        self.summary_writer = tf.summary.create_file_writer(\n",
    "            self.log_dir + \"/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        )\n",
    "    \n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Load and preprocess the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            train_dataset: TensorFlow dataset for training\n",
    "            test_dataset: TensorFlow dataset for testing\n",
    "        \"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        images, conditions = load_dataset(self.dataset_path)\n",
    "        \n",
    "        print(f\"Found {len(images)} images.\")\n",
    "        if len(images) == 0:\n",
    "            raise ValueError(\"No images found in the dataset.\")\n",
    "        \n",
    "        print(\"Preprocessing dataset...\")\n",
    "        segmentation_maps, conditions = preprocess_dataset(images, conditions)\n",
    "        \n",
    "        # Split into training and testing sets\n",
    "        X_train, X_test, y_train, y_test, cond_train, cond_test = train_test_split(\n",
    "            images, segmentation_maps, conditions, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {len(X_train)} images, Test set: {len(X_test)} images\")\n",
    "        \n",
    "        # Create TensorFlow datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            {'image': X_train, 'condition': cond_train}, y_train\n",
    "        )).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "        \n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            {'image': X_test, 'condition': cond_test}, y_test\n",
    "        )).batch(BATCH_SIZE)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, input_image, target, condition):\n",
    "        \"\"\"\n",
    "        Training step for the GAN.\n",
    "        \n",
    "        Args:\n",
    "            input_image: Input image\n",
    "            target: Target segmentation map\n",
    "            condition: Condition vector\n",
    "            \n",
    "        Returns:\n",
    "            gen_loss: Generator loss\n",
    "            disc_loss: Discriminator loss\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # Generate fake image\n",
    "            gen_output = self.generator([input_image, condition], training=True)\n",
    "            \n",
    "            # Discriminator outputs\n",
    "            disc_real_output = self.discriminator([input_image, condition], training=True)\n",
    "            disc_generated_output = self.discriminator([gen_output, condition], training=True)\n",
    "            \n",
    "            # Calculate losses\n",
    "            gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        generator_gradients = gen_tape.gradient(\n",
    "            gen_loss, self.generator.trainable_variables\n",
    "        )\n",
    "        discriminator_gradients = disc_tape.gradient(\n",
    "            disc_loss, self.discriminator.trainable_variables\n",
    "        )\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_variables)\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, self.discriminator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        return gen_loss, disc_loss\n",
    "    \n",
    "    def train(self, epochs=EPOCHS):\n",
    "        \"\"\"\n",
    "        Train the GAN.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of epochs\n",
    "        \"\"\"\n",
    "        train_dataset, test_dataset = self.load_and_preprocess_data()\n",
    "        \n",
    "        # Display model summaries\n",
    "        self.generator.summary()\n",
    "        self.discriminator.summary()\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            start = datetime.now()\n",
    "            \n",
    "            # Training\n",
    "            for batch_idx, (batch_inputs, batch_targets) in enumerate(tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "                input_image = batch_inputs['image']\n",
    "                condition = batch_inputs['condition']\n",
    "                \n",
    "                gen_loss, disc_loss = self.train_step(input_image, batch_targets, condition)\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx} - \"\n",
    "                          f\"Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}\")\n",
    "                    \n",
    "                    # Log to TensorBoard\n",
    "                    with self.summary_writer.as_default():\n",
    "                        tf.summary.scalar('gen_loss', gen_loss, step=epoch * len(train_dataset) + batch_idx)\n",
    "                        tf.summary.scalar('disc_loss', disc_loss, step=epoch * len(train_dataset) + batch_idx)\n",
    "            \n",
    "            # Save checkpoint every 10 epochs (or more frequently for shorter training)\n",
    "            if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "                self.checkpoint.save(file_prefix=self.checkpoint_prefix)\n",
    "                \n",
    "                # Generate and save sample images\n",
    "                self.generate_and_save_images(epoch + 1, test_dataset)\n",
    "            \n",
    "            time_taken = datetime.now() - start\n",
    "            print(f\"Time taken for epoch {epoch+1}: {time_taken}\")\n",
    "            \n",
    "            # Early stopping check (optional)\n",
    "            # if gen_loss < early_stopping_threshold:\n",
    "            #     print(f\"Early stopping at epoch {epoch+1} as loss threshold reached\")\n",
    "            #     break\n",
    "        \n",
    "        # Save the final model\n",
    "        generator_path = os.path.join(OUTPUT_DIR, 'generator_model.h5')\n",
    "        discriminator_path = os.path.join(OUTPUT_DIR, 'discriminator_model.h5')\n",
    "        \n",
    "        self.generator.save(generator_path)\n",
    "        self.discriminator.save(discriminator_path)\n",
    "        \n",
    "        print(f\"Models saved to {OUTPUT_DIR}\")\n",
    "        \n",
    "        # Generate final samples\n",
    "        self.generate_and_save_images(epochs, test_dataset)\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return generator_path\n",
    "    \n",
    "    def generate_and_save_images(self, epoch, test_dataset):\n",
    "        \"\"\"\n",
    "        Generate and save sample floor plans.\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch\n",
    "            test_dataset: Test dataset\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        output_dir = os.path.join(OUTPUT_DIR, f'generated_samples/epoch_{epoch}')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Take a sample from the test dataset\n",
    "        for batch_inputs, _ in test_dataset.take(1):\n",
    "            input_image = batch_inputs['image']\n",
    "            \n",
    "            # Generate images for each condition type\n",
    "            for i, plot_type in enumerate(PLOT_TYPES):\n",
    "                # Create condition vector for this plot type\n",
    "                cond = np.zeros((1, len(PLOT_TYPES)))\n",
    "                cond[0, i] = 1\n",
    "                \n",
    "                # Generate floor plan\n",
    "                prediction = self.generator([input_image[:1], cond], training=False)\n",
    "                \n",
    "                # Convert prediction to segmentation map\n",
    "                pred_map = tf.argmax(prediction[0], axis=-1)\n",
    "                \n",
    "                # Convert segmentation map to RGB image\n",
    "                rgb_image = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
    "                \n",
    "                for y in range(IMG_HEIGHT):\n",
    "                    for x in range(IMG_WIDTH):\n",
    "                        class_idx = pred_map[y, x].numpy()\n",
    "                        if class_idx < len(INDEX_TO_COLOR):\n",
    "                            rgb_image[y, x] = INDEX_TO_COLOR[class_idx]\n",
    "                \n",
    "                # Save the generated image\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(rgb_image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Generated {plot_type} Floor Plan')\n",
    "                plt.savefig(os.path.join(output_dir, f'{plot_type}_sample.png'))\n",
    "                plt.close()\n",
    "                \n",
    "                # Display in the notebook\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.imshow(rgb_image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Generated {plot_type} Floor Plan (Epoch {epoch})')\n",
    "                plt.show()\n",
    "    \n",
    "    def generate_floor_plan(self, plot_type, seed=None, num_samples=1):\n",
    "        \"\"\"\n",
    "        Generate floor plans for a specific plot type.\n",
    "        \n",
    "        Args:\n",
    "            plot_type: Type of plot ('5_marla', '10_marla', or '20_marla')\n",
    "            seed: Random seed for reproducibility\n",
    "            num_samples: Number of samples to generate\n",
    "            \n",
    "        Returns:\n",
    "            rgb_images: List of generated floor plans as RGB images\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            tf.random.set_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        rgb_images = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Create random noise\n",
    "            noise = np.random.normal(0, 1, (1, IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS))\n",
    "            \n",
    "            # Create condition vector\n",
    "            condition = np.zeros((1, len(PLOT_TYPES)))\n",
    "            plot_idx = PLOT_TYPES.index(plot_type)\n",
    "            condition[0, plot_idx] = 1\n",
    "            \n",
    "            # Generate floor plan\n",
    "            prediction = self.generator([noise, condition], training=False)\n",
    "            \n",
    "            # Convert prediction to segmentation map\n",
    "            pred_map = tf.argmax(prediction[0], axis=-1)\n",
    "            \n",
    "            # Convert segmentation map to RGB image\n",
    "            rgb_image = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
    "            \n",
    "            for y in range(IMG_HEIGHT):\n",
    "                for x in range(IMG_WIDTH):\n",
    "                    class_idx = pred_map[y, x].numpy()\n",
    "                    if class_idx < len(INDEX_TO_COLOR):\n",
    "                        rgb_image[y, x] = INDEX_TO_COLOR[class_idx]\n",
    "            \n",
    "            rgb_images.append(rgb_image)\n",
    "            \n",
    "            # Display the generated floor plan\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(f'Generated {plot_type} Floor Plan - Sample {i+1}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        return rgb_images\n",
    "    \n",
    "    def export_floor_plan(self, rgb_image, output_path, format='png'):\n",
    "        \"\"\"\n",
    "        Export a floor plan to file.\n",
    "        \n",
    "        Args:\n",
    "            rgb_image: Floor plan as RGB image\n",
    "            output_path: Path to save the floor plan\n",
    "            format: Output format ('png', 'svg', or 'dxf')\n",
    "        \"\"\"\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        if format == 'png':\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            print(f\"Floor plan exported to {output_path}\")\n",
    "            \n",
    "        elif format == 'svg':\n",
    "            # Basic SVG export\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(output_path, format='svg', bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            print(f\"Floor plan exported to {output_path}\")\n",
    "            \n",
    "        elif format == 'dxf':\n",
    "            # DXF export requires more complex conversion\n",
    "            # For Colab, we'll save as PNG instead\n",
    "            print(\"DXF export feature requires additional implementation with a library like ezdxf.\")\n",
    "            print(\"Saving as PNG instead for now.\")\n",
    "            output_png = output_path.replace('.dxf', '.png')\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(output_png, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            print(f\"Floor plan exported to {output_png}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage_section"
   },
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "Now, let's train the floor plan generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "source": [
    "# Check if the dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise ValueError(f\"Dataset not found at {DATASET_PATH}. Please upload your dataset to Google Drive or check the path.\")\n",
    "\n",
    "# Print dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "for plot_type in PLOT_TYPES:\n",
    "    plot_dir = os.path.join(DATASET_PATH, plot_type)\n",
    "    if os.path.exists(plot_dir):\n",
    "        files = [f for f in os.listdir(plot_dir) if f.endswith('.png')]\n",
    "        print(f\"  - {plot_type}: {len(files)} images\")\n",
    "\n",
    "# Initialize the floor plan generator\n",
    "print(\"\\nInitializing Floor Plan Generator...\")\n",
    "generator = FloorPlanGenerator(DATASET_PATH)\n",
    "\n",
    "# Set a lower number of epochs for demonstration\n",
    "training_epochs = 50  # You can increase this for better results\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nStarting training for {training_epochs} epochs...\")\n",
    "generator_path = generator.train(epochs=training_epochs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate_section"
   },
   "source": [
    "## 6. Generating Floor Plans\n",
    "\n",
    "Let's generate some floor plans for each plot type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_plans"
   },
   "source": [
    "# Generate floor plans for each plot type\n",
    "for plot_type in PLOT_TYPES:\n",
    "    print(f\"\\nGenerating floor plans for {plot_type}:\")\n",
    "    \n",
    "    # Generate 3 samples for this plot type\n",
    "    floor_plans = generator.generate_floor_plan(plot_type, num_samples=3)\n",
    "    \n",
    "    # Export the floor plans in different formats\n",
    "    for i, floor_plan in enumerate(floor_plans):\n",
    "        # Create output directory\n",
    "        output_dir = os.path.join(OUTPUT_DIR, plot_type)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Export as PNG\n",
    "        generator.export_floor_plan(\n",
    "            floor_plan, \n",
    "            os.path.join(output_dir, f\"floor_plan_{i+1}.png\"), \n",
    "            format='png'\n",
    "        )\n",
    "        \n",
    "        # Export as SVG\n",
    "        generator.export_floor_plan(\n",
    "            floor_plan, \n",
    "            os.path.join(output_dir, f\"floor_plan_{i+1}.svg\"), \n",
    "            format='svg'\n",
    "        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "legend_section"
   },
   "source": [
    "## 7. Create Color Legend\n",
    "\n",
    "Let's create a legend for the color-coded room types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_legend"
   },
   "source": [
    "def create_legend():\n",
    "    \"\"\"\n",
    "    Create a legend for the color-coded room types.\n",
    "    \"\"\"\n",
    "    # Create a figure for the legend\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create patches for legend\n",
    "    patches = []\n",
    "    labels = []\n",
    "    \n",
    "    for room_type, color in COLOR_MAP.items():\n",
    "        # Convert RGB color to matplotlib format (0-1 range)\n",
    "        color_normalized = [c / 255 for c in color]\n",
    "        \n",
    "        # Create a patch and add it to the list\n",
    "        patch = plt.Rectangle((0, 0), 1, 1, fc=color_normalized)\n",
    "        patches.append(patch)\n",
    "        labels.append(room_type)\n",
    "    \n",
    "    # Create the legend\n",
    "    plt.legend(patches, labels, loc='center', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.title('Floor Plan Color Legend', fontsize=16)\n",
    "    \n",
    "    # Save the legend\n",
    "    legend_path = os.path.join(OUTPUT_DIR, 'color_legend.png')\n",
    "    plt.savefig(legend_path)\n",
    "    plt.show()\n",
    "    print(f\"Color legend saved to {legend_path}\")\n",
    "\n",
    "# Create the color legend\n",
    "create_legend()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_to_drive_section"
   },
   "source": [
    "## 8. Save Results to Drive\n",
    "\n",
    "Let's save the trained model and generated floor plans to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "source": [
    "def save_to_drive(local_path, drive_path):\n",
    "    \"\"\"\n",
    "    Save files from Colab to Google Drive.\n",
    "    \n",
    "    Args:\n",
    "        local_path: Path to the local files in Colab\n",
    "        drive_path: Path to save the files in Google Drive\n",
    "    \"\"\"\n",
    "    # Create the directory in Drive if it doesn't exist\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "    \n",
    "    # Copy the files\n",
    "    !cp -r {local_path}/* {drive_path}/\n",
    "    \n",
    "    print(f\"Files copied from {local_path} to {drive_path}\")\n",
    "\n",
    "# Set the path in Google Drive to save the results\n",
    "drive_output_path = '/content/drive/MyDrive/FYP-FP-Generator-Model/output'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(drive_output_path, exist_ok=True)\n",
    "\n",
    "# Save the results to Drive\n",
    "save_to_drive(OUTPUT_DIR, drive_output_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion_section"
   },
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "You have successfully implemented and trained a floor plan generator model! \n",
    "\n",
    "### What We've Accomplished:\n",
    "\n",
    "1. Loaded and preprocessed the floor plan dataset\n",
    "2. Built a Conditional GAN model architecture\n",
    "3. Trained the model to generate floor plans based on plot type\n",
    "4. Generated sample floor plans for each plot type\n",
    "5. Saved the results to Google Drive\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Train the model for more epochs to improve results\n",
    "2. Implement more sophisticated evaluation metrics\n",
    "3. Add constraints to ensure architectural validity\n",
    "4. Develop a more interactive interface for floor plan generation\n",
    "\n",
    "The trained model and generated floor plans are now available in your Google Drive at: `{drive_output_path}`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Floor Plan Generator Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
